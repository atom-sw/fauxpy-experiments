---
title: "Fault localization experiments in Python"
author: "Mohammad Rezaalipour and Carlo A. Furia"
header-includes:
   - \usepackage[T1]{fontenc}
   - \usepackage[scaled=0.81]{beramono}
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    df_print: paged
link-citations: yes
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, cache=TRUE, cache.lazy=FALSE)
```

```{r render, eval=FALSE, include=FALSE}
## Evaluate this snippet to knit this document.
rmarkdown::render("pyfl.Rmd")
```

```{r include=FALSE}

# Assertions
library(assertthat)

# Regex and string manipulation
library(stringr)

# Powerful table manipulation
library(dplyr)
# Disable warning when summarize groups by default
options(dplyr.summarise.inform=FALSE)
library(tidyr)
## Data reshaping documentation: https://uc-r.github.io/tidyr

# Fancy, easy plots
library(ggplot2)
# More complicated plots
library(GGally)
## ggpairs extensive documentation: https://ggobi.github.io/ggally/articles/ggpairs.html

# Beautiful, readable color palettes
library(MetBrewer)

# Various effect size calculations
library(effsize)

# Bayesian inference
library(posterior)
library(brms)
library(rethinking)


## For determinism
MY.SEED <- 73544204
set.seed(MY.SEED)
# No need to thread safe RNGs
options(future.rng.onMisuse="ignore")
# Use available CPUs
options(mc.cores=parallel::detectCores())
# Set cmdstanr as backend for brms if you have it
options(brms.backend="cmdstanr")



# Colors for projects
P.PALETTE <- "Renoir"
P.COLORS <- met.brewer(P.PALETTE, n=12)

# Colors for categories
C.PALETTE <- "Archambault"
C.COLORS <- met.brewer(C.PALETTE, n=4)
```

```{r include=FALSE}

family <- "/pyfl/"
stored.values <- list()
stored.values.str <- list()

PAPER.DIR <- file.path("data")
PAPER.FNAME <- "data.tex"

STORED.NUMS <- list()
STORED.STRINGS <- list()

sanitize_ <- function(name)
{
  res <- name
  res <- gsub("&", "+", res)
  res <- gsub("%", "p", res)
  res
}

# Store `value` under `key` as numeric value.
# If `as.string` store it as string value instead.
# Use `family/group` as a hierarchical prefixes.
store_ <- function(key, value, group="", family=paste0("pyfl", DATASAMPLE), as.string=FALSE)
{
  maybe.group <- if (group == "") "" else paste0(group, "/")
  key <- sanitize_(paste0("/", family, "/", maybe.group, key))
  if (as.string)
    STORED.STRINGS[[key]] <<- seqinr::stresc(as.character(value))
  else {
    STORED.NUMS[[key]] <<- value
  }
}

# Store all content of table `df` as numeric or string values,
# under keys of the form `row.ids`:`col.ids`, using `tabname` as the group name.
#
# If `row.ids` is NULL and `df`'s row names are unique, use them as row identifiers;
# if `row.ids` is NULL and `df`'s row names are NOT unique, use row numbers as row identifiers.
# Argument `col.ids` works as `row.ids` but for column names.
#
# If `replace.probs` is TRUE and row identifiers are unique, change 
# row identifiers that denote lower bounds "|0.p" to "lower p0", 
# and row identifiers that denote upper bounds "0.p|" to "upper p0".
#
# If `vectorize` is TRUE and `df` consists of a single column, use keys 
# of the form `row.ids` (i.e., a single index instead of a pair of indexes).
store.table_ <- function(df, tabname, replace.probs=FALSE, row.ids=NULL, col.ids=NULL, vectorize=TRUE)
{
  if (is.null(row.ids)) {
    if (any(duplicated(rownames(df))))
      row.ids <- 1:nrow(df)
    else
      row.ids <- rownames(df)
  }
  if (is.null(col.ids)) {
    if (any(duplicated(colnames(df))))
      col.ids <- 1:ncol(df)
    else
      col.ids <- colnames(df)
  }
  if (replace.probs && !any(duplicated(row.ids))) {
    mo <- (row.ids %>% str_match("^[|]([0-9]+[.]?[0-9]*)$"))[, 2]
    row.ids <- ifelse(is.na(mo), row.ids, paste0("lower ", 100*as.numeric(mo)))
    mo <- (row.ids %>% str_match("^([0-9]+[.]?[0-9]*)[|]$"))[, 2]
    row.ids <- ifelse(is.na(mo), row.ids, paste0("upper ", 100*as.numeric(mo)))
  }
  for (row in 1:length(row.ids)) {
    for (col in 1:length(col.ids)) {
      if (length(col.ids) == 1 && vectorize)
        key <- paste0(row.ids[row])
      else
        key <- paste0(row.ids[row], ":", col.ids[col])
      value <- df[row, col]
      store_(key, value, group=tabname, as.string=!is.numeric(value))
    }
  }
}

# Write all numeric constants to `filename` in `out_dir`.
# If `strings`, write all string constants instead.
# If `reset`, empty the content of the variable store that has been dumped.
dump.constants_ <- function(filename, out_dir="", strings=FALSE, append=FALSE, reset=FALSE)
{
   fn <- file.path(out_dir, filename)
   if (strings)
     to.store <- STORED.STRINGS
   else
     to.store <- STORED.NUMS
   constants <- paste0("\\pgfkeyssetvalue{", names(to.store), "}{", unlist(to.store), "}")
   write_lines(constants, fn, append=append)
   if (reset) {
     if (strings)
       STORED.STRINGS <<- list()
    else
      STORED.NUMS <<- list()
   }
}

# Write all stored constants into default file
dump.all_ <- function(filename=PAPER.FNAME, out_dir=PAPER.DIR, append=TRUE, reset=TRUE)
{
  dump.constants_(filename, out_dir=out_dir, strings=FALSE, append=append)
  dump.constants_(filename, out_dir=out_dir, strings=TRUE, append=append)
}
```



# Data input

Let's start with reading the computed metrics for all projects.

```{r data input, include=TRUE}
metrics.dir <- "../metric_computation/output/"
assertthat::assert_that(dir.exists(metrics.dir))

GRANULARITIES <- factor(c("function", "statement"))
FAMILIES <- list(mbfl=c("Metallaxis", "Muse"), 
                 ps=c("PS"), 
                 st=c("ST"),
                 sbfl=c("DStar", "Ochiai", "Tarantula"))
  
datas <- data.frame()
for (granularity in GRANULARITIES) {
  pat <- str_c("^detailed_", granularity, "_(.*)[.]csv$")
  infiles <- list.files(path=metrics.dir, pattern=pat, ignore.case=TRUE)
  for (fname in infiles) {
    technique <- str_extract(fname, regex(pat, ignore_case=TRUE), group=1)
    family <- names(which(sapply(FAMILIES, function(x) technique %in% x)))
    cur.data <- read.csv(file.path(metrics.dir, fname))
    datas <- datas %>% bind_rows(cbind(
      granularity=granularity, 
      family=family, 
      technique=technique, 
      cur.data
      ))
  }
}

# convert to factors relevant columns
datas$family <- factor(datas$family)
datas$technique <- factor(datas$technique)
# rename to avoid - character
datas$project_name[datas$project_name == "youtube-dl"] <- "youtubedl"
datas$project_name <- factor(datas$project_name)

TECHNIQUES <- unique(datas$technique)

# show some data
str(datas)
```

We have a total of `r length(unique(datas$project_name))` projects, which we can classify by category:

   - Data science (`ds`): `spacy`, `keras`, `pandas`
   - Developer tools (`dev`): `cookiecutter`, `black`
   - Web tools (`web`): `sanic`, `fastapi`, `tornado`
   - Command line tools (`cl`): `httpie`, `thefuck`, `tqdm`, `youtubedl`
   
Let's add this categorization to the data.

```{r categories}
CATEGORIES <- list(ds=c("spacy", "keras", "pandas"),
                   dev=c("cookiecutter", "black"),
                   web=c("sanic", "fastapi", "tornado"),
                   cl=c("httpie", "thefuck", "tqdm", "youtubedl"))

# assign category to each datapoint according to its project
categories <- sapply(datas$project_name, 
                     function(project)
                       names(which(sapply(CATEGORIES,
                                          function(categories) 
                                            project %in% categories))))
datas$category <- categories
```

# Pairwise comparisons

Let's see an example of visual and statistical comparison of two groups of experiments for the same bugs.

To make the example concrete, let's pick two groups and compare their $E_{\text{inspect}}$ scores on statement-level fault localization:

   - $S$ are the experiments done with any SBFL technique
   - $M$ are the experiments done with any MBFL technique

Since there are three experiments per bug using SBFL, 
but only two experiments per bug using MBFL,
we'll aggregate scores for the same bug by average.


```{r group aggregation}
statement.by.family <- (datas 
                        %>% filter(granularity == "statement")
                        # force order of families as in names(FAMILIES)
                        %>% mutate(family=factor(family, levels=names(FAMILIES))))
statement.by.family <- (statement.by.family 
                        %>% group_by(family, category, project_name, bug_number) 
                        %>% summarize(
                          e_inspect=mean(e_inspect),
                          time=mean(experiment_time_seconds),
                          exam=mean(exam_score)
                        ))

# one column per family with its e_inspect score
statement.by.family.e_inspect <- (statement.by.family 
                                  # remove other metrics
                                  %>% select(-time, -exam)
                                  %>% spread(family, e_inspect))

S <- statement.by.family.e_inspect$sbfl 
M <- statement.by.family.e_inspect$mbfl
```

Let's start with some visualization: a scatterplot with a point for each bug;
each point has coordinates $x, y$ where $x$ is its score in MBFL and $y$ its score in SBFL.

```{r scatterplot}
p <- ggplot(statement.by.family.e_inspect, 
            aes(x=mbfl, y=sbfl, color=category)) + geom_point(size=3)
p <- p + scale_color_met_d(C.PALETTE) + theme_classic()

# x = y reference line
p <- p + geom_abline(slope=1, intercept=0)

p
```

As you can see, 
there are a bunch of bugs for which SBFL performs very similarly to MBFL 
(points close to the $x = y$ straight line).
However, for several other bugs, SBFL is much better (remember that lower is better for this score).

Looking at the colors, which denote the project categories, it seems data science (and possibly web frameworks) are overrepresented among the "harder" bugs on which SBFL behaves consistenly better than MBFL.

Analyzing the same data numerically, we can compute the correlation (Kendall's $\tau$) between $S$ and $M$:

```{r correlation}
corr <- cor.test(S, M, method="kendall")
corr
```

A correlation of `r corr$estimate` is not insignificant, but it is not strong either.

Finally, we may also perform a statistical test (Wilcoxon's paired test)
and compute a matching effect size (Cliff's delta).

```{r tests}
wilcox.test(S, M, paired=TRUE)

cliff.delta(S, M)
```

Cliff's delta, in particular, measures how often the value in one set are larger than the value in the other set.
Thus, the given value means that SBFL's $E_{\text{inspect}}$ score is smaller than MBFL's roughly in 40\% of the cases.


These statistics, for what they're worth,
seem to confirm that there is a noticeable difference in favor of SBFL.

Now, let's generalize this to a scatterplot matrix to show the relations between all possible pairs of FL families.

```{r scatterplot matrix, echo=FALSE, message=FALSE, results='hide', fig.keep='all'}
#' Compute statistics comparing data `x` and `y`.
#'
#' The function returns a vector of strings, each expressing a statistics and
#' some kind of strength level with stars.
#'
#' @param x   A numeric vector with the first dataset.
#' @param y   A numeric vector with the second dataset.
#' @param digits   The number of decimal digits of the statistics.
#' @param corr.method   The correlation method to be used (see `cor.test`).
#' @param conf.level   The confidence level used for the significance tests.
#'
#' @return  Returns a vector of strings with correlation, p-value of 
#'          a Wilcoxon paired test, and Cliff's delta.
#'
pairs_text_fn <- function(x, y, digits, corr.method, conf.level)
{
  alpha <- 1 - conf.level
  # correlation
  corObj <- stats::cor.test(x, y, method=corr.method, conf.level=conf.level)
  cor_est <- as.numeric(corObj$estimate)
  cor_txt <- formatC(cor_est, digits=digits, format="f")
  cor_sig <- corObj$p.value < alpha
  cor_abs <- abs(cor_est)
  cor_stars <- ifelse(cor_abs <= 0.3, 0,
                      ifelse(cor_abs <= 0.5, 1,
                             ifelse(cor_abs <= 0.7, 2,
                                    3)))
  cor_txt <- str_c(cor_txt, strrep("*", cor_stars))
  # Wilcoxon
  sigObj <- wilcox.test(x, y, paired=TRUE, conf.level=conf.level)
  sig_est <- as.numeric(sigObj$p.value)
  sig_txt <- formatC(sig_est, digits=digits, format="f")
  sig_sig <- sigObj$p.value < alpha
  if (sig_sig)
    sig_txt <- str_c(sig_txt, signif_stars(sigObj$p.value, point=NULL))
  # Cliff's delta
  deltaObj <- cliff.delta(x, y, conf.level=conf.level)
  delta_est <- as.numeric(deltaObj$estimate)
  delta_txt <- formatC(delta_est, digits=digits, format="f")
  delta_sig <- deltaObj$magnitude
  n_stars <- switch(as.character(delta_sig),
                    "negligible"=0,
                    "small"=1,
                    "medium"=2,
                    "large"=3)
  delta_txt <- str_c(delta_txt, strrep("*", n_stars))
  # add \n at the end of each statistic
  sapply(c(cor_txt, sig_txt, delta_txt), function(s) str_c(s, "\n"))
}

#' Helper function wrapping `pairs_text_fn` in a call to 
#' `GGally::ggally_statistics`, providing sensible defaults.
pairs.stats <- function (data, mapping, ..., 
                         digits=2, na.rm=TRUE, 
                         corr.method="kendall", conf.level=0.95)
{
    ggally_statistic(data=data, 
                     mapping=mapping, 
                     title=c("Corr", "p-value", "Effect"),
                     na.rm=na.rm,
                     justify_text="left", justify_labels="left", sep=": ",
                     title_args=list(color="black"),
                     text_fn=function(x, y) pairs_text_fn(x, y, 
                                                          digits, 
                                                          corr.method, 
                                                          conf.level))
}

#' Helper function to add a line x = y to a ggpairs plot.
points_abline <- function(data, mapping)
{
  pp <- ggplot(data=data, mapping=mapping) + geom_point()
  pp <- pp + geom_abline(intercept=0, slope=1)
  pp
}

#' Pairwise comparison plot with scatterplots on lower, bar charts on diagonal, 
#' and statistics on upper.
#' 
#' @param data  The dataset used for plotting.
#' @param columns  The indexes of columns to be compared pairwise.
#' @param color_var  The column used to group the plotted data, as a string.
#' @param palette  The name of the MetBrewer palette used to color groups.
#'
#' @return  Returns a ggplot2 plot object. 
pairwise.comparison.plot <- function(data, columns, 
                                     color_var="category", palette=NULL)
{
  pp <- ggpairs(data, columns, 
                legend=1,
                lower=list(mapping=aes(color=.data[[color_var]]),
                           continuous=wrap(points_abline)),
                diag=list(continuous=wrap("barDiag", bins=15),
                          mapping=aes(color=.data[[color_var]])),
                upper=list(continuous=wrap(pairs.stats)))
  if (!is.null(palette))
    pp <- pp + scale_color_met_d(palette) + scale_fill_met_d(palette)
  pp <- pp + theme_light()
  # change theme selectively for upper cells
  # this only works if no other theme is set in the rest of the plot
  # pp <- add_to_ggmatrix(pp, 
  #                       # this "casting" is needed in my version of GGally
  #                       ggproto(ggplot2::theme_minimal()), 
  #                       location="upper")
  return(pp)
}

p.e_inspect <- pairwise.comparison.plot(
  statement.by.family.e_inspect, 
  columns=which(names(statement.by.family.e_inspect) %in% names(FAMILIES)),
  color_var="category", palette=C.PALETTE)
p.e_inspect
```

Now, it's easy to compute a similar plot for other metrics.
First, running time.

```{r time, echo=FALSE, message=FALSE, results='hide', fig.keep='all'}

# one column per family with its time
statement.by.family.time <- (statement.by.family 
                             # remove other metrics
                             %>% select(-e_inspect, -exam)
                             %>% spread(family, time))
p.time <- pairwise.comparison.plot(
  statement.by.family.time, 
  columns=which(names(statement.by.family.time) %in% names(FAMILIES)),
                color_var="category", palette=C.PALETTE
  )
p.time <- p.time + theme(axis.text.x=element_text(angle=90))
p.time
```

Then, exam score.

```{r exam score, echo=FALSE, message=FALSE, results='hide', fig.keep='all'}
# one column per family with its time
statement.by.family.exam <- (statement.by.family 
                             # remove other metrics
                             %>% select(-e_inspect, -time)
                             %>% spread(family, exam))
p.exam <- pairwise.comparison.plot(
  statement.by.family.exam, 
  columns=which(names(statement.by.family.exam) %in% names(FAMILIES)),
                color_var="category", palette=C.PALETTE
  )
p.exam
```



# Regression models

Let's build a simple multivariate regression model, 
where we predict `e_inspect` and `time` from bug and technique.

First, we need a unique identifier for every bug, 
which combines project and bug number within that project.
We also standardize the predictors, so that it's much easier to set sensible priors.

```{r bug identifiers}
statement <- (datas 
              %>% filter(granularity=="statement") 
              %>% rename(time=experiment_time_seconds))
bugs <- str_c(statement$project_name, statement$bug_number)
bugs <- factor(bugs)
statement$bug <- bugs

statement$timeS <- (statement$time - mean(statement$time)) / sd(statement$time)
statement$einspectS <- (statement$e_inspect - mean(statement$e_inspect)) / sd(statement$e_inspect)
```

## Model $m_1$

Here's a basic regression model, where the only unusual aspects
are that it's multivariate, and log-transforms the mean (since both outcome variables must be nonnegative).

```{r m1 formula}
eq.m1 <- brmsformula(
  mvbind(einspectS, timeS) ~ 0 + technique + bug,
  family=brmsfamily("gaussian", link="log")
) + set_rescor(TRUE)

pp1.check <- get_prior(eq.m1, data=statement)

pp1 <- c(
  set_prior("normal(0, 1.0)", class="b", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 1)", class="sigma", resp=c("einspectS", "timeS"))
)

eq.m1g <- brmsformula(
  mvbind(einspectS, timeS) ~ 0 + family + category,
  family=brmsfamily("gaussian", link="log")
) + set_rescor(TRUE)

pp1g.check <- get_prior(eq.m1g, data=statement)

pp1g <- c(
  set_prior("normal(0, 1.0)", class="b", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 1)", class="sigma", resp=c("einspectS", "timeS"))
)
```


## Fitting $m_1$

Let's do the usual checks to make sure that everything is fine with the fitting.

Prior checks, confirming that the sampled priors span a wide range of values, 
amply including the data.

```{r m1 priors check, warning=FALSE, message=FALSE, error=FALSE, results='hide'}
m1.priors <- brm(eq.m1,
                 prior=pp1,
                 data=statement,
                 chains=1, iter=1000,
                 sample_prior="only",
                 refresh=0, 
                 control=list(adapt_delta=0.95),
                 seed=MY.SEED)

(prior1.check.einspect <- pp_check(m1.priors, draw_ids=1:100, resp="einspectS") + scale_x_log10())
(prior1.check.time <- pp_check(m1.priors, draw_ids=1:100, resp="timeS") + scale_x_log10())

m1g.priors <- brm(eq.m1g,
                  prior=pp1g,
                  data=statement,
                  chains=1, iter=1000,
                  sample_prior="only",
                  refresh=0, 
                  control=list(adapt_delta=0.95),
                  seed=MY.SEED)

(prior1g.check.einspect <- pp_check(m1g.priors, draw_ids=1:100, resp="einspectS") + scale_x_log10())
(prior1g.check.time <- pp_check(m1g.priors, draw_ids=1:100, resp="timeS") + scale_x_log10())
```

Now we fit the actual model.

```{r m1 fit}
m1 <- brm(eq.m1, data=statement, prior=pp1, refresh=0, control=list(adapt_delta=0.95), seed=MY.SEED)

m1g <- brm(eq.m1g, data=statement, prior=pp1g, refresh=0, control=list(adapt_delta=0.95), seed=MY.SEED)
```

Next, we check the usual diagnostics:

   - No (or at most a few) divergent transitions
   - $\widehat{R}$ ratio below $1.01$
   - Effective sample size (ESS), as a ratio of the total sample size, at least 10%

```{r m1 diagnostics}

# Divergent transitions
np <- nuts_params(m1)
print(sum(subset(np, Parameter == "divergent__")$Value))

# \hat{R} ratio
print(max(brms::rhat(m1), na.rm = TRUE))

# ESS
min(neff_ratio(m1g), na.rm = TRUE)

# Divergent transitions
np <- nuts_params(m1g)
print(sum(subset(np, Parameter == "divergent__")$Value))

# \hat{R} ratio
print(max(brms::rhat(m1g), na.rm = TRUE))

# ESS
min(neff_ratio(m1g), na.rm = TRUE)
```

Finally, we check the posteriors, to ensure that we have a decent approximation of the data.

```{r m1 posterior checks, warning=FALSE, message=FALSE}
print(pp_check(m1, ndraws=100, resp="einspectS") + scale_x_log10())
print(pp_check(m1, ndraws=100, resp="timeS") + scale_x_log10())

print(pp_check(m1g, ndraws=100, resp="einspectS") + scale_x_log10())
print(pp_check(m1g, ndraws=100, resp="timeS") + scale_x_log10())
```

As you can see, the simulated posteriors are not exactly a great fit, 
but this is understandable given that the data is complex, whereas the model is quite simple.

The posterior check is a tad better in the "grouped" models though,
which makes sense since grouping makes for coarser dependencies to model.


## Analyzing $m_1$


```{r m1 summary}
summary(m1)
summary(m1g)
```
What's noticeable here is some non-negligible residual correlation
between the two outcomes `einspect` and `time`, which means that there
is a dependency between these two variables that is not captured by the model.

This residual correlation is much smaller in the "grouped" model, 
but still noticeable.

Let's set up some functions to analyze the posterior samples of `m1` (and similar models).

```{r m1 difference analysis defs, warning=FALSE}
PROBABILITIES <- c(0.5, 0.7, 0.9, 0.95, 0.99)

#' Given a technique, which family it belongs to
which.family <- function(t) names(FAMILIES)[sapply(FAMILIES, function(x) t %in% x)]

#' Given a bug unique identifier, which category of programs it belongs to
which.projcategory <- function(t) 
  names(CATEGORIES)[sapply(CATEGORIES, 
                           function(x) any(str_detect(t, x)))]

PROJECTS <- unique(datas$project_name)
BUGS <- unique(statement$bug)

interval.results <- function(fit, response, which.group, predictor)
{
  # posterior draws
  post <- posterior::as_draws_df(fit)
  # get columns of coefficients corresponding to `response` and `predictor`
  post <- post[, grep(str_c("^b_", response, "_", predictor), colnames(post))]
  # overall mean of posterior draws
  grand.mean <- mean(unlist(post))
  # difference to mean
  diffs <- apply(post, 2, function(x) x - grand.mean)
  # keep only the predictor's names
  colnames(diffs) <- str_extract(colnames(diffs), 
                                 str_c("^b_", response, "_", predictor, "(.*)$"),
                                 group=1)
  # reshape
  diffs <- data.frame(diffs) %>% gather(predictor, response)
  # add group column
  diffs$group <- sapply(diffs$predictor, which.group)
  # compute HPDIs
  intervals <- (diffs
                %>% group_by(group) 
                %>% reframe(response=HPDI(response, prob=rev(PROBABILITIES)), 
                            prob=names(HPDI(response, prob=rev(PROBABILITIES)))))
  # reshape back and sort in inclusion interval order
  intervals <- intervals %>% spread(group, response)
  # sort in inclusion interval order
  nrow.2 <- round(nrow(intervals)/2)
  res <- rbind(intervals[1:nrow.2, ], intervals[nrow(intervals):(nrow.2+1), ])
  res
}
```

Let's use these functions to first analyze the effects per *family*
of FL techniques.

```{r m1 per family, warning=FALSE}
(res.m1.einspect.family <- interval.results(m1, "einspectS", 
                                            which.family, "technique"))
(res.m1.time.family <- interval.results(m1, "timeS", 
                                        which.family, "technique"))

(res.m1g.einspect.family <- interval.results(m1g, "einspectS", 
                                             identity, "family"))
(res.m1g.time.family <- interval.results(m1g, "timeS", 
                                         identity, "family"))
```
It's clear that for both outcomes, `e_inspect` and `time`,
there are strong differences (with high probability) 
in the contribution over the mean from different techniques.

In contrast, grouping the effects by category of project does not
yeld any consistent difference, as all intervals overlap zero 
(and are often nearly centered around it). 
There is a slight tendency for projects in categories `cl` and `web` to lead
to shorter running times, but it's a very weak tendency.

```{r m1 per category, warning=FALSE}
(res.m1.einspect.category <- interval.results(m1, "einspectS", 
                                              which.projcategory, "bug"))
(res.m1.time.category <- interval.results(m1, "timeS", 
                                          which.projcategory, "bug"))

(res.m1g.einspect.category <- interval.results(m1g, "einspectS", 
                                              identity, "category"))
(res.m1g.time.category <- interval.results(m1g, "timeS", 
                                           identity, "category"))
```

## Model $m_2$

Let's make the model more sophisticated, with varying effects,
and modeling these effects as [possibly correlated](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html) 
(which makes sense,
since we have two model parts)

```{r m2 formula}
eq.m2 <- brmsformula(
  mvbind(einspectS, timeS) ~ 1 + (1|p|technique) + (1|q|bug),
  family=brmsfamily("gaussian", link="log")
) + set_rescor(TRUE)

pp2.check <- get_prior(eq.m2, data=statement)

pp2 <- c(
  set_prior("normal(0, 1.0)", class="Intercept", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 0.3)", class="sd", coef="Intercept", 
            group="technique", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 0.3)", class="sd", coef="Intercept", 
            group="bug", resp=c("einspectS", "timeS")),
  set_prior("gamma(0.01, 0.01)", class="sigma", resp=c("einspectS", "timeS"))
)

eq.m2g <- brmsformula(
  mvbind(einspectS, timeS) ~ 1 + (1|p|family) + (1|q|category),
  family=brmsfamily("gaussian", link="log")
) + set_rescor(TRUE)

pp2g.check <- get_prior(eq.m2g, data=statement)

pp2g <- c(
  set_prior("normal(0, 1.0)", class="Intercept", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 0.3)", class="sd", coef="Intercept", 
            group="family", resp=c("einspectS", "timeS")),
  set_prior("weibull(2, 0.3)", class="sd", coef="Intercept", 
            group="category", resp=c("einspectS", "timeS")),
  set_prior("gamma(0.01, 0.01)", class="sigma", resp=c("einspectS", "timeS"))
)
```


## Fitting $m_2$

Let's fit $m_2$ and check the fit.

Prior checks:

```{r m2 priors check, warning=FALSE, message=FALSE, error=FALSE, results='hide'}
m2.priors <- brm(eq.m2,
                 prior=pp2,
                 data=statement,
                 chains=1, iter=1000,
                 sample_prior="only",
                 refresh=0, 
                 control=list(adapt_delta=0.95),
                 seed=MY.SEED)

(pp_check(m2.priors, draw_ids=1:100, resp="einspectS") + scale_x_log10())
(pp_check(m2.priors, draw_ids=1:100, resp="timeS") + scale_x_log10())

m2g.priors <- brm(eq.m2g,
                  prior=pp2g,
                  data=statement,
                  chains=1, iter=1000,
                  sample_prior="only",
                  refresh=0, 
                  control=list(adapt_delta=0.95),
                  seed=MY.SEED)

(pp_check(m2g.priors, draw_ids=1:100, resp="einspectS") + scale_x_log10())
(pp_check(m2g.priors, draw_ids=1:100, resp="timeS") + scale_x_log10())
```

We fit model $m_2$.

```{r m2 fit}
m2 <- brm(eq.m2, data=statement, prior=pp2, 
          refresh=0,
          control=list(adapt_delta=0.95), 
          seed=MY.SEED)

m2g <- brm(eq.m2g, data=statement, prior=pp2g, 
           refresh=0,
           control=list(adapt_delta=0.95), 
           seed=MY.SEED)
```

Diagnostics:

```{r m2 diagnostics}

# Divergent transitions
np <- nuts_params(m2)
print(sum(subset(np, Parameter == "divergent__")$Value))

# \hat{R} ratio
print(max(brms::rhat(m2), na.rm = TRUE))

# ESS
min(neff_ratio(m2), na.rm = TRUE)

# Divergent transitions
np <- nuts_params(m2g)
print(sum(subset(np, Parameter == "divergent__")$Value))

# \hat{R} ratio
print(max(brms::rhat(m2g), na.rm = TRUE))

# ESS
min(neff_ratio(m2g), na.rm = TRUE)
```

Posterior checks:

```{r m2 posterior checks, warning=FALSE, message=FALSE}
print(pp_check(m2, ndraws=100, resp="einspectS") + scale_x_log10())
print(pp_check(m2, ndraws=100, resp="timeS") + scale_x_log10())

print(pp_check(m2g, ndraws=100, resp="einspectS") + scale_x_log10())
print(pp_check(m2g, ndraws=100, resp="timeS") + scale_x_log10())
```

We don't notice a massive improvement compared to $m_1$.
Let's compare the two models using LOO.

```{r model comparison, warning=FALSE}
m1 <- add_criterion(m1, criterion = "loo")
m2 <- add_criterion(m2, criterion = "loo")

(lc.m1m2 <- loo(m1, m2, moment_match = TRUE))

m1g <- add_criterion(m1g, criterion = "loo")
m2g <- add_criterion(m2g, criterion = "loo")

(lc.m1gm2g <- loo(m1g, m2g, moment_match = TRUE))
```

$m_1$'s score is around 4.6 standard deviations worse than $m_2$'s,
which is a clear difference in favor of $m_2$ in terms of predictive
accuracy.

The difference is less conspicuous for the "grouped" models, but it's
still 2.3 standard deviations, which is certainly not negligible.

## Analyzing $m_2$


```{r m2 summary}
summary(m2)
summary(m2g)
```
Both with `technique` and `bug`, the two outcomes `einspect` and `time`
are positively correlated (a stronger effect with `technique`). 
The residual correlation is smaller compared to `m_1`, which justifies using
a varying effect model and is consistent with the LOO comparison.

Let's now perform an effects analysis 
on the fitted coefficients of `m2`.
First we introduce a summary function.

```{r m2 difference analysis defs, warning=FALSE}
interval.results.ranef <- function(fit, response, predictor, probs)
{
  # probability intervals and their labels
  ps.low <- rev(1-probs)/2
  ps.high <- rev(1-ps.low)
  ps <- rev(c(ps.high, ps.low))
  cnams <- c(paste("|", probs, sep=""), paste(rev(probs), "|", sep=""))
  # random effects
  post <- ranef(fit, probs=ps)
  # keep only predictor/response data
  post <- post[[predictor]]
  post <- post[, , grep(str_c("^", response), dimnames(post)[[3]])]
  res <- as.data.frame(post)
  res <- res[, grep("^Q", colnames(res))]
  colnames(res) <- cnams
  res <- t(res)
  est <- ranef(fit, probs=ps)[[predictor]][, "Estimate", 1]
  list(ints=res, est=est)
}
```

Then we use the functions to analyze the effects
of the FL techniques.

```{r m2 per family, warning=FALSE}
(res.m2.einspect.technique <- interval.results.ranef(
  m2, "einspectS",  "technique", PROBABILITIES
 ))
(res.m2.time.technique <- interval.results.ranef(
  m2, "timeS",  "technique", PROBABILITIES
 ))

(res.m2g.einspect.family <- interval.results.ranef(
  m2g, "einspectS",  "family", PROBABILITIES
  ))
(res.m2g.time.family <- interval.results.ranef(
  m2g, "timeS",  "family", PROBABILITIES
  ))
```
The results are in line with those obtained with the simpler model,
but they are now less clear-cut.

Let's see what happens for the bug/category of projects.

```{r m2 per category, warning=FALSE}
(res.m2.einspect.bug <- interval.results.ranef(
  m2, "einspectS", "bug", PROBABILITIES)
 )
(res.m2.time.bug <- interval.results.ranef(
  m2, "timeS", "bug", PROBABILITIES)
  )

(res.m2g.einspect.category <- interval.results.ranef(
  m2g, "einspectS", "category", PROBABILITIES)
 )
(res.m2g.time.category <- interval.results.ranef(
  m2g, "timeS", "category", PROBABILITIES)
  )
```


# Dump all constants

```{r epilogue}
# dump.all_()
```